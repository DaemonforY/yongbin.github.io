

本章结合淘宝ODS具体实例中遇到的一些问题以及这些问题解决方案的实现思路，综合归纳了ODS生产实践中的常见问题和挑战：实时和准实时数据需求、数据飘移处理、巨型数据量表处理、如何有效控制数据存储。

# 实时和准实时数据需求

随着日益发展的业务，传统的只能获取T-1时间粒度数据的数据仓库已经越来越难以满足用户的看数需求，特别像淘宝双11这样的大促，卖家需要实时分析数据来调整促销力度以提高销量。为了满足这类实时数据的需求，我们需要建立实时数据仓库,这个概念最早是Michaem Haisten 提出的，他对实时数据仓库的分类和架构有详细的描述。

实时数据仓库的主要思想就是：在数据仓库中，将保存的数据分为两类，一种为静态数据，一种为动态数据，静态数据满足用户的查询分析要求；而动态数据就是为了适应实时性，数据源中发生的更新可以立刻传送到数据仓库的动态数据中，再经过响应的转换，满足实时的要求。

现在更多的实时分析是建立在ODS上而不是数据仓库上，因为ODS处理逻辑简单，数据链路相对较短，产出更快。根据数据刷新频率的不同，可以分为3类ODS：

- 实时ODS
  这类ODS 的数据可以同步或接近同步地和业务系统数据同时刷新,实现实时的运营决策,例如实时的反欺诈、淘宝双11大促实时大屏。这要求ODS 数据的抽取、转换和加载也是实时进行的。这一类ODS一般是通过实时的消息传递中间件来实现,要求涉及的业务过程不能过多，处理的业务逻辑不能过于复杂。这一类ODS 的建设成本最高。

  

- 准实时ODS
  这类ODS 的数据可以基本上实现与业务系统的同步,例如15分钟到1小时刷新一次,这类ODS比实时ODS成本要低些，基本可以满足大部分的实时需求。并且可以根据实际需求调整刷新频率，具有较好的灵活性。

  

- 传统ODS
  这是目前为止最常见的ODS ,其数据一天刷新一次,可以利用业务系统的空闲时间进行刷新（通常是每天凌晨0-2点）,可实现所有业务系统的数据集成和刷新。刷新频率的下降也给系统有更多的时间进行数据更正和清洗。第三类是最容易建设也最便宜的一种ODS。

  

关于对准实时的ODS数据处理，我们将会遇到一个问题：多个准实时ODS表关联，可能会遗漏跨天的数据。例如，交易小时表和优惠券领取的小时表关联，存在当天支付成功的订单，但是优惠券是昨天领用的，这样如果以交易作为主表就会缺失优惠券领取的信息，如果以优惠券领取小时表作为主表，就会缺失优惠券使用的交易订单。

针对这种情况，我们可以采用几种处理方案：

（1）最佳的方案是直接和业务系统沟通，将有关联的业务过程发生时同时更新对应的表，例如，淘宝的下单表、支付表、物流订单表，在用户下单时支付表和物流订单表会同时新增记录；当用户在支付时，下单表和物流订单表的订单状态业务随之发生变化；在用户购买的宝贝确认收货时，下单表和支付表的状态也会随之变更。这样我们在处理交易全流程的小时表时，只需要将三个表关联就可以了，不会发生漏数的问题。

（2）还有一种方法是，我们在ODS层生成截止当前小时全量表，将小时表和天粒度T-1的ODS表作merge操作生成，这样后续的处理就不会发生漏数，但是这种处理的计算和存储成本都比较高。

（3）对于部分缓慢变化的维度表，我们可以直接用天粒度T-1的ODS表关联来处理。

在做处理这类准实时的ODS表时，需要特别注意ETL任务的产出效率，通常这类任务的产出时间最多不能超过ODS表的刷新周期时间。例如小时级别的表，任务不能超过1个小时。



# 数据飘移处理

数据飘移是ODS数据的一个顽疾，通常是指ODS表的同一个业务日期数据中包含前一天或后一天凌晨附近的数据或者丢失当天的变更数据。

由于ODS需要承接面向历史的细节数据查询需求，这就需要物理落地到数据仓库的ODS表按时间段来切分进行分区存储，通常的做法是按某些时间戳字段来切分，实际往往由于时间戳字段的准确性问题导致数据飘移问题的发生。

通常来说，时间戳字段分为四类：

- 数据库表中用来标示数据记录更新的时间戳字段（假设这类字段叫modified_time）
- 数据库日志中标示数据记录的更新时间的时间戳字段（假设这类字段叫log_time）
- 数据库表中的用来记录具体业务过程的发生时间（假设这类字段叫proc_time）
- 数据记录被抽取到的时间（假设这类字段叫extract_time）

理论上，这几个时间应该是一致的，但是实际生产中，这几个时间往往会出现差异：

- 由于数据抽取是需要时间的，extract_time往往会晚于前三个时间
- 前台业务系统手工订正数据时未更新modified_time
- 由于网络或者系统压力问题，log_time或者modified_time会晚于proc_time

我们通常的做法是根据其中的某一个字段来切分ODS表，这就导致了数据飘移的产生，下面我们来具体看下数据飘移的几种场景：

- 根据extract_time来获取数据
  这种情况数据飘移的问题最明显
- 根据modified_time限制
  这种情况再实际生产中用的最多，但是往往就会遇到不更新modified_time而导致的数据遗漏，或者凌晨时间的产生的数据记录飘移到后一天
- 根据log_time限制
  由于网络或者系统压力问题，log_time会晚于proc_time，从而导致凌晨时间的产生的数据记录飘移到后一天。例如，在淘宝双11大促时间凌晨产生的数据量非常大，用户支付需要调用多个接口，从而导致支付时间小于记录更新时间
- 根据proc_time限制
  仅仅根据某一个proc_time限制，我们所获取的ODS表仅仅包含一个业务过程所产生的记录，会遗漏很多其他过程的变化记录，这和我们ODS和业务系统保持一致这个设计原则有所违背。

处理方法主要有以下两种：

- 多获取后一天的数据
  这种方式相对比较简单，既然很难做到数据飘移的问题，我就在ODS每个时间分区中向前向后都多冗余一些数据，保障我的数据只会多不会少，具体的数据切分让下游根据自身不同的业务场景根据不同的业务时间proc_time来限制。 但是这种方式会有一些数据误差，例如一个订单是当天支付的，但是第二天凌晨申请退款关闭了该条订单，该条记录的订单状态会被更新，下游再统计支付订单状态时会错误统计。
- 多个时间戳字段限制时间来获取相对准确的数据
  （1）首先根据log_time分别冗余前一天最后15分钟的数据和后一天凌晨开始15分钟的数据，并modified_time过滤非当天数据，确保数据不会因为系统问题遗漏数据；
  （2）再根据log_time取后一天的15分钟数据；针对此数据，按照主键根据log_time作升序排列去重。因为我们需要获取的最接近当天记录变化情况（数据库日志数据将保留所有变化的数据，但是落地到ODS的表是需要根据主键去重获取最后状态的变化情况）；
  （3）最后将前两步的结果数据作全外连接，限定业务时间proc_time来获取我们需要的数据。

下面来看下，处理淘宝交易订单的数据飘移的实际案例：

我们在处理双11交易订单的时候发现，有一大批在11-11 23:59:59左右支付的交易订单飘移到了12号。主要原因是用户下单支付后系统需要调用支付宝的接口而有所延迟，导致这些订单最终生成的时间跨天了。即modified_time和log_time都晚于proc_time。

如果订单只有一个支付的业务过程，我们可以用支付时间来限制就能取到正确的数据。但是往往实际订单有多个业务过程：下单、支付、成功，每个业务过程都有相应的时间字段，并不仅仅是支付会飘移。

如果我们直接通过多获取后一天的数据，然后限制这些时间可以获取到相关数据，但是存在一个和前面第一种方法相同的问题，后一天的数据可能已经更新多次，我们直接获取到的那条记录已经是更新多次后的状态，数据准确性存在一定的问题。

因此，我们根据实际情况获取后一天15分钟的数据，并限制多个业务过程的时间字段（下单、支付、成功）都是双11当天的，然后对这些数据按照订单的modified_time做升序排列，获取每条订单首次数据变更的那条记录。

另一方面，我们根据log_time分别冗余前一天最后15分钟的数据和后一天凌晨开始15分钟的数据，并modified_time过滤非当天数据，针对每条订单按照log_time降序排列，取每条订单当天的最后一次数据变更的那条记录。

最后将两份数据根据订单做FULL OUTER JOIN，将漂移数据回补到当天数据中。

数据逻辑表示如下（红色表示实际不可能存在的情况）：

![ods_3_2-1](http://aligitlab.oss-cn-hangzhou-zmf.aliyuncs.com/uploads/yongwei.wangyw/model/034daf85c69d859ef2f8542edfc4cbe2/ods_3_2-1.png)



# 巨型表的处理

在电商数据仓库中，存在非常多的数据量上百亿级别的巨型表，这些表不是那些只有新增没有更新逻辑的日志表，而是存在着数据记录的变更。这些巨型表在做全量存储时，需要做几亿增量merge几百亿全量加上下游应用访问时都需要消耗非常多的系统计算资源。这类巨型表我们需要根据实际应用场景做一些特殊处理。

例一：物流订单表，在生产实践中我们发现：

- 绝大部分下游应用都是基于最近200天创建的订单；
- 只有在极为特殊的情况才会出现增量数据更新200天以前的订单；
- 绝大部分下游应用并不关心200天以前的订单

所以我们采用一种采用分而治之的方法，将部分历史数据从源表中剥离出来单独生成表，具体处理方法如下图所示：

![ods_3_3-1](http://aligitlab.oss-cn-hangzhou-zmf.aliyuncs.com/uploads/yongwei.wangyw/model/4560ff6a09e2dc93d11b4ebab473a4eb/ods_3_3-1.png) 其中：

- 全量表：全量存储，每个分区存储最近200天创建的订单；

- OLD表：增量存储，以订单创建时间作为分区字段，存储200天以前的创建的订单；

- 极限存储表：对最近200天的全量存储做极限存储

  这样做的好处是在存储、计算、历史数据可获取等方面做了一个均衡。

例二：商品表，在生产实践中我们发现：

- 商品数据和现实中的商品一样存在生命周期

- 由于迭代周期短，全量商品表中大部分都是失效的商品

- 失效的商品数据和现实下架的商品一样访问频率极聚下降

  所以我们还是采用一种采用分而治之的方法，将失效的商品数据从源表中剥离出来单独生成表。 ![ods_3_3-2](http://aligitlab.oss-cn-hangzhou-zmf.aliyuncs.com/uploads/yongwei.wangyw/model/ec5f673476669b9b92060ddbb42d0370/ods_3_3-2.png)



# 3.4 有效控制数据存储

由于ODS对接的业务系统的明细数据，数据量在整个数据仓库体系中是最大的，如何做到有效控制数据存储就显得尤为重要。在长期的生产实际过程中，我们总结一些方法和经验：

1. 去除重复的表
   重复的表从来源上主要可能是：
   （1）多个数据团队各自为政，如果集团公司内部存在多个数据团队，往往会有多个数据团队负责不同的数据集市或者业务，有可能存在各自重复同步同源数据（同一个库下的同一个表）；
   （2）权限松散，烟囱式开发，不同的开发人员都可以直接从前端同步数据开发报表；
   解决这类问题的首要措施不是技术上而是管理上的，必须建立统一的ODS层，收拢权限，由专门的团队统一管控。这类同源头数据重复同步的任务和表需要下线。但是也有例外情况，例如业务需要的时间粒度是不同，虽然是同一个表但是时间切片不一致。在淘宝就存在国际站和中文站公用一个表的情况，国际站的业务需要在下午18点开始同步数据，而中文站的业务需要在0点左右同步数据。

   

2. 设置合理生命周期
   当我们采用全量存储时，往往历史分区的访问频率是不高的，有些甚至是一段时间完全没有访问，因此需要设置合理的生命周期，将那些没有访问需求的分区删除。

   

3. 去除无价值的数据
   存储是有成本的，而并不是所有的数据都是有价值的，有些无价值的数据存储在系统中只是白白消耗成本，因此我们需要将部分无价值的数据删除：
   （1）超过一段时间无人访问的ODS表（可再生的）下线
   考虑到互联网行业业务发展速度，往往超过一段时间（一般3个月）无人访问，说明这块数据在当前及相当一对时间之内对业务已经不能产出价值了，需要下线
   （2）源系统无数据更新表下线
   这类表往往是源系统已经切库（只剩下历史数据了）或者业务已经下线了，当然也需要判断下是否缓慢变化的维表。
   （3）产出为空的表需要review后下线

   

4. 压缩历史分区数据
   类似日志、交易流水这类ODS表的历史分区，一方面比较重要属于不可再生的重要资产，另一方面既不会有大批量的访问需求，又有零星访问。所以既不能粗暴的删除从而导致资损，也不能备份到其他地方导致下游访问不畅。比较好的的做法就是将这些历史分区数据进行压缩，虽然会有部分访问性能的缺失，但是可以带来更大的存储成本的节约。

   

5. 备份数据
   不可再生的重要历史数据并且也没有相应的新数据产出了，即不能直接删除，仍然存储在生产应用中也不合适，将会影响其他数据的治理策略。这类数据我们需要将其压缩备份到备份库中。

   

这些异常表都可以通过元数据分析自动获取，然后推送给owner来处理，或者通过脚本批量处理。